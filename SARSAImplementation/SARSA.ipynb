{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random as random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA Algorithm Implementation for Maze\n",
    "- Here we implement the SARSA algorithm to find an optimum path through a square maze\n",
    "- There are several *pitfalls* - places where the agent cannot go, and only one exit\n",
    "- We punish pitfalls with $-50$, and reward finding the exit with $10^5$\n",
    "- We use a $\\epsilon$-greedy policy with $\\epsilon(t) = \\frac{0.4}{1+10^{-4}t}$\n",
    "- At each step of the algorihtm, the action-value function is updated according to\n",
    "$q(s,a) \\to q(s,a) - \\alpha(r(s') + \\gamma q(s',a') - q(s,a))$\n",
    "- Here $(s,a)$ are the current state and action choice. $s'$ is the state we will go to next under action $a$, and $a'$ is the action the greed-policy algo would choose at this current instance in state $s'$.\n",
    "- If we hit a wall, the action-value function is updated but the state is not: we stay put and try again\n",
    "- $\\alpha$ is the learning rate and $\\gamma$ is the discount factor (incentives high reward actions in the near future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing the maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = 5 #maze is a length x length box\n",
    "pitfalls = [np.array((0,1)),np.array((1,3)),\n",
    "            np.array((1,4)),np.array((2,2)),\n",
    "            np.array((3,3)),np.array((4,3)),\n",
    "            np.array((4,4))]\n",
    "\n",
    "start = np.array((4,0))\n",
    "end = np.array((0,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agent is the algorithm which moves around the board.\n",
    "    We initialize the agent with an initial position.\n",
    "    It has internal clock t which measures its number of steps taken\n",
    "    The action_val_funcs determines probabilistically which policy the agent will take, it is updated in each experiment\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self,initial_pos):\n",
    "        self.initial_pos = np.array(initial_pos)\n",
    "        self.current_pos = np.array(initial_pos)\n",
    "        self.t=0\n",
    "        self.action_val_funcs = np.zeros((length*length,4)) #num states x num actions\n",
    "        self.finished = False\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.1\n",
    "        \n",
    "        \n",
    "    def _epsilon(self):\n",
    "        return(0.4/(1+(10**-4)*self.t))\n",
    "        \n",
    "    def _find_best_choice(self,state):\n",
    "        \"\"\"Given a state, this identifies the best action\n",
    "        If multiple have the same reward, it chooses a random one.\n",
    "        \"\"\"\n",
    "        index = self._convert_state_to_matrix_index(state)\n",
    "        find_max_indices = np.argwhere(self.action_val_funcs[index] == np.amax(self.action_val_funcs[index]))\n",
    "        if np.size(find_max_indices) == 1:\n",
    "            return(np.argmax(self.action_val_funcs[index]))\n",
    "        else: \n",
    "            return random.choice(find_max_indices)[0]\n",
    "            \n",
    "        \n",
    "    def _greedy_choice(self,state):\n",
    "        \"\"\"Implement the epsilon greedy policy: choose the best policy (1-epsilon) times.\n",
    "        \"\"\"\n",
    "        best_option = self._find_best_choice(state)\n",
    "        if random.random() <= (1-self._epsilon()):\n",
    "            return best_option\n",
    "        else:\n",
    "            return(random.choice(np.delete(np.array(range(4)),best_option)))\n",
    "    \n",
    "    def _get_action(self,index):\n",
    "        options = [np.array((1,0)),np.array((-1,0)),np.array((0,1)),np.array((0,-1))]\n",
    "        return options[index]\n",
    "    \n",
    "    def _convert_state_to_matrix_index(self,state):\n",
    "        #convert state (a,b) to 0,..lengthxlength-1 matrix index\n",
    "        return state[0]+length*state[1]\n",
    "    \n",
    "    def _get_penalty(self,pos):\n",
    "        if np.array_equal(pos,end):\n",
    "            self.finished = True\n",
    "            return 10**5\n",
    "        if np.any(np.all(pos == pitfalls, axis=1)):\n",
    "            self.finished = True\n",
    "            return -50\n",
    "        if pos[0] <0 or pos [0] >length-1 or pos[1] <0 or pos[1] >length-1:\n",
    "            return -2\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "        \n",
    "    def move_step(self):\n",
    "        \"\"\"Moves the agent one step in its environment\n",
    "        Computes penalty and updates action-value functions based on new data\n",
    "        \"\"\"\n",
    "        action_choice = self._greedy_choice(self.current_pos) \n",
    "        \n",
    "        new_pos = self.current_pos + self._get_action(action_choice)\n",
    "        \n",
    "        penalty = self._get_penalty(new_pos)\n",
    "        state_index = self._convert_state_to_matrix_index(self.current_pos)\n",
    "        new_state_index = self._convert_state_to_matrix_index(new_pos)\n",
    "                        \n",
    "        if penalty == -2 or penalty == -50: #then new state is old state\n",
    "            new_action_choice = self._greedy_choice(self.current_pos)\n",
    "            self.action_val_funcs[state_index][action_choice] += self.alpha *(penalty + (self.gamma*self.action_val_funcs[state_index][new_action_choice])\n",
    "                                                                              - self.action_val_funcs[state_index][action_choice])                                 \n",
    "        else:   \n",
    "            new_action_choice = self._greedy_choice(new_pos)\n",
    "            self.action_val_funcs[state_index][action_choice] += self.alpha *(penalty + (self.gamma*self.action_val_funcs[new_state_index][new_action_choice])\n",
    "                                                                              - self.action_val_funcs[state_index][action_choice])\n",
    "                                                                           \n",
    "        if penalty != -2: #update to new_pos unless illegal move.\n",
    "            self.current_pos = new_pos\n",
    "        self.t +=1\n",
    "        \n",
    "        \n",
    "    def _reset(self):\n",
    "        self.current_pos = np.array(self.initial_pos)\n",
    "        self.t =0\n",
    "        self.finished = False\n",
    "        \n",
    "    def do_experiment(self):\n",
    "        while self.t < 1000 and self.finished != True:\n",
    "            self.move_step()\n",
    "        self._reset()\n",
    "    \n",
    "    def do_experiment_of_length_L(self,L):\n",
    "        while self.t < L and self.finished !=True:\n",
    "            self.move_step()\n",
    "        self._reset()\n",
    "    \n",
    "    def get_action_val_func(self):\n",
    "        return(self.action_val_funcs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent((4,0))\n",
    "for i in range(1000):\n",
    "    agent.do_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Outcome\n",
    "- The action-value function takes the form of a matrix. Each entry shows the \"reward\" of choosing a particular action given a particular state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26624.68733271,  9273.24797948,  3989.57071342,  5221.29554506],\n",
       "       [20970.32151493, 15694.93261534, 40102.7418863 , 27793.25382699],\n",
       "       [19245.16437225, 29523.2665254 , 30318.92658173, 23496.3522064 ],\n",
       "       [15226.88866781, 25398.60091857, 20159.38954819, 19355.50484595],\n",
       "       [15262.70378069, 20077.3602207 , 16656.01796288, 15866.8579092 ],\n",
       "       [    0.        ,     0.        ,     0.        ,     0.        ],\n",
       "       [31083.53655922, 43320.49049783, 55628.20698675, 21202.72257781],\n",
       "       [21689.01617837, 41239.93468692, 29171.17080115, 23114.58523602],\n",
       "       [15021.92290463, 32364.21590105,  9307.41640291, 19730.71340709],\n",
       "       [11762.70546143, 25107.83461293,  5425.90153487, 14406.60517137],\n",
       "       [51193.08925123, 63470.68564348, 82742.48747292, 66140.52138912],\n",
       "       [52452.61954037, 67108.75811386, 52992.05202135, 35330.54011644],\n",
       "       [    0.        ,     0.        ,     0.        ,     0.        ],\n",
       "       [ 2265.89195589,  3761.68501222,  6115.07613983, 16751.75542503],\n",
       "       [ 2275.53127563,  2403.41044443,   309.19289088, 10999.90591634],\n",
       "       [79809.55741395, 74597.1655181 , 99999.99998823, 63315.49166728],\n",
       "       [    0.        ,     0.        ,     0.        ,     0.        ],\n",
       "       [    0.        ,     0.        ,     0.        ,     0.        ],\n",
       "       [    0.        ,     0.        ,     0.        ,     0.        ],\n",
       "       [    0.        ,     0.        ,     0.        ,     0.        ],\n",
       "       [    0.        ,     0.        ,     0.        ,     0.        ],\n",
       "       [    0.        ,     0.        ,     0.        ,     0.        ],\n",
       "       [    0.        ,     0.        ,     0.        ,     0.        ],\n",
       "       [    0.        ,     0.        ,     0.        ,     0.        ],\n",
       "       [    0.        ,     0.        ,     0.        ,     0.        ]])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = agent.get_action_val_func()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Outcome to Directionality\n",
    "- We convert the above matrix notation into the diagram which demonstrates what the agent has learned.\n",
    "- We see that in each position of the matrix, it has identified a best action to take\n",
    "- The blank options are cases where the algorithm has reached no determination (either the region is out of bounds, or a pitfall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', '', '', '', ''],\n",
       " ['up', '', '', '', ''],\n",
       " ['up', 'left', '', 'down', 'down'],\n",
       " ['', 'up', 'left', 'left', 'left'],\n",
       " ['right', 'up', 'up', 'left', 'left']]"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_output_to_diagram(output):\n",
    "    results = [[\"\",\"\",\"\",\"\",\"\"],[\"\",\"\",\"\",\"\",\"\"],[\"\",\"\",\"\",\"\",\"\"],[\"\",\"\",\"\",\"\",\"\"],[\"\",\"\",\"\",\"\",\"\"]]\n",
    "    for i in range(length**2):\n",
    "        translate = {0:\"right\",1:\"left\",2:\"up\",3:\"down\"}\n",
    "        if np.array_equal(output[i], np.array((0,0,0,0))):\n",
    "            results[length-1-int(i/length)][i%length] == \"n/a\"\n",
    "        else:\n",
    "            results[length-1-int(i/length)][i%length] = translate[np.argmax(output[i])]\n",
    "        \n",
    "    return(results)\n",
    "\n",
    "convert_output_to_diagram(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jennyenv)",
   "language": "python",
   "name": "jennyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
